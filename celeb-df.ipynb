{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:13:27.291727Z","iopub.status.busy":"2024-05-12T15:13:27.291330Z","iopub.status.idle":"2024-05-12T15:13:27.302250Z","shell.execute_reply":"2024-05-12T15:13:27.301108Z","shell.execute_reply.started":"2024-05-12T15:13:27.291698Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","import os\n","import time\n","import pandas as pd\n","from PIL import Image\n","\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import seaborn as sns\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, TimeDistributed, Dense, Dropout, BatchNormalization\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","\n","import warnings\n","import random\n","from tqdm.notebook import tqdm\n","warnings.filterwarnings('ignore')\n","%matplotlib inline\n","\n","from tensorflow.keras.utils import to_categorical\n","from keras.preprocessing.image import load_img\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:13:27.318911Z","iopub.status.busy":"2024-05-12T15:13:27.318235Z","iopub.status.idle":"2024-05-12T15:13:27.353369Z","shell.execute_reply":"2024-05-12T15:13:27.352217Z","shell.execute_reply.started":"2024-05-12T15:13:27.318846Z"},"trusted":true},"outputs":[],"source":["# DATASET PATHS\n","\n","# TRAIN_DIR = \"D:\\Zeina\\Documents\\GUC\\8th Semester\\Datasets\\Celeb-DF\\extract_frames\\Train\"\n","# TEST_DIR = 'D:\\Zeina\\Documents\\GUC\\8th Semester\\Datasets\\Celeb-DF\\extract_frames\\Test'\n","# VAL_DIR = 'D:\\Zeina\\Documents\\GUC\\8th Semester\\Datasets\\Celeb-DF\\extract_frames\\Validation'\n","\n","\n","# TRAIN_DIR = \"/content/drive/MyDrive/Celeb-DF/Kaggle_Train\"\n","# VAL_DIR = \"/content/drive/MyDrive/Celeb-DF/Validation\"\n","\n","DATA_DIR = '/kaggle/input/celeb-df-v2'\n","\n","face_detector=cv2.CascadeClassifier('/kaggle/input/haar-cascades-for-face-detection/haarcascade_frontalface_default.xml')\n","\n","img_size = 64\n","seq_length = 50\n","step = 10\n","batch_size = 32"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:13:27.354832Z","iopub.status.busy":"2024-05-12T15:13:27.354523Z","iopub.status.idle":"2024-05-12T15:13:27.360574Z","shell.execute_reply":"2024-05-12T15:13:27.358491Z","shell.execute_reply.started":"2024-05-12T15:13:27.354807Z"},"trusted":true},"outputs":[],"source":["# img = cv2.imread('/kaggle/input/test-img/IMG_0953.JPG')\n","# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","# results = face_detector.detectMultiScale(gray, 1.3, 5)\n","\n","# for (x,y,w,h) in results:\n","#     cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n","#     faces = img[y:y + h, x:x + w] \n","#     cv2.imshow(\"face\",faces) \n","  \n","# # cv2.imshow('img',img)\n","# cv2.waitKey(0)\n","# cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:13:27.363635Z","iopub.status.busy":"2024-05-12T15:13:27.363172Z","iopub.status.idle":"2024-05-12T15:13:27.373222Z","shell.execute_reply":"2024-05-12T15:13:27.372092Z","shell.execute_reply.started":"2024-05-12T15:13:27.363598Z"},"trusted":true},"outputs":[],"source":["def detect_face(img):\n","#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_detector.detectMultiScale(img, 1.3, 5)\n","    \n","    #if at least one face detected\n","    if(len(faces) > 0):\n","        for (x,y,w,h) in faces:\n","            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n","            face = img[y:y + h, x:x + w] #crop frame to face\n","            return face\n","    else:\n","        return img #if no face detected use frame as is\n","#         cv2.imshow(\"face\",faces) "]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:13:27.375096Z","iopub.status.busy":"2024-05-12T15:13:27.374753Z","iopub.status.idle":"2024-05-12T15:13:27.386534Z","shell.execute_reply":"2024-05-12T15:13:27.385313Z","shell.execute_reply.started":"2024-05-12T15:13:27.375067Z"},"trusted":true},"outputs":[],"source":["def load_frames_from_video(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        face = detect_face(frame)  \n","        frame = cv2.resize(face, (img_size, img_size))\n","        frame = frame.astype('float32') / 255.0\n","        frames.append(frame)\n","    cap.release()\n","\n","    # Pad or slice frames to have a consistent sequence length\n","    if len(frames) < seq_length:\n","        frames += [np.zeros((img_size, img_size, 3))] * (seq_length - len(frames))\n","    frames = frames[:seq_length]\n","\n","    return np.array(frames)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T16:33:21.868188Z","iopub.status.busy":"2024-05-12T16:33:21.867812Z","iopub.status.idle":"2024-05-12T16:33:21.875722Z","shell.execute_reply":"2024-05-12T16:33:21.874548Z","shell.execute_reply.started":"2024-05-12T16:33:21.868160Z"},"trusted":true},"outputs":[],"source":["def setup_dataset(root_path, real_files, fake_files, batch_size, is_training):\n","    frames, labels = [], []\n","    for category in (\"Celeb-real\", \"Celeb-synthesis\"):\n","        folder_path = os.path.join(root_path, category)\n","        if(category == \"Celeb-real\"):\n","            files = real_files\n","        else:\n","            files = fake_files\n","        # videos = os.listdir(folder_path)\n","        for video_name in tqdm(files):\n","            video_path = os.path.join(folder_path, video_name)\n","            print(\"video_path\", video_path)\n","            vid_frames = load_frames_from_video(video_path)\n","            frames.append(vid_frames)\n","            labels.append(0 if category == \"Celeb-real\" else 1)\n","\n","#     dataset = tf.data.Dataset.from_tensor_slices((frame_paths, labels))\n","#     if is_training:\n","#         dataset = dataset.shuffle(buffer_size=100)\n","#     dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","#     return dataset\n","\n","    return frames, labels"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:13:27.423034Z","iopub.status.busy":"2024-05-12T15:13:27.422677Z","iopub.status.idle":"2024-05-12T15:13:27.451134Z","shell.execute_reply":"2024-05-12T15:13:27.450017Z","shell.execute_reply.started":"2024-05-12T15:13:27.422990Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Real Files before dataset reduction: 590\n","Fake Files before dataset reduction: 5639\n","---------------------------------------------\n","Real Files after 50% reduction: 295\n","Fake Files after 50% reduction: 2820\n","---------------------------------------------\n","TESTING SET (20%)\n","Real Test Files: 59\n","Fake Test Files: 564\n","---------------------------------------------\n","VALIDATION SET (10%)\n","Real Validation Files: 24\n","Fake Validation Files: 226\n","---------------------------------------------\n","TRAINING SET (80%)\n","Real Train Files: 169\n","Fake Train Files: 1624\n","---------------------------------------------\n","Total Train Files: 1793\n","Total Test Files: 623\n","Total Validation: 250\n"]}],"source":["##### Splitting the dataset into train, validation, and test sets\n","from sklearn.model_selection import train_test_split\n","\n","real_files = os.listdir(os.path.join(DATA_DIR, 'Celeb-real'))\n","fake_files = os.listdir(os.path.join(DATA_DIR, 'Celeb-synthesis'))\n","\n","print(f\"Real Files before dataset reduction: {len(real_files)}\")\n","print(f\"Fake Files before dataset reduction: {len(fake_files)}\")\n","\n","print(\"---------------------------------------------\")\n","\n","real_extra, real_sample = train_test_split(real_files, test_size=0.5, random_state=42)\n","fake_extra, fake_sample = train_test_split(fake_files, test_size=0.5, random_state=42)\n","\n","print(f\"Real Files after 50% reduction: {len(real_sample)}\")\n","print(f\"Fake Files after 50% reduction: {len(fake_sample)}\")\n","\n","print(\"---------------------------------------------\")\n","\n","real_train, real_test = train_test_split(real_sample, test_size=0.2, random_state=42)    # test size = 20%\n","real_train, real_val = train_test_split(real_train, test_size=0.1, random_state=42)     # validation size = 10% of train (80%) = 8%\n","real_train, real_extra = train_test_split(real_train, test_size=0.2, random_state=42)\n","\n","fake_train, fake_test = train_test_split(fake_sample, test_size=0.2, random_state=42)   \n","fake_train, fake_val = train_test_split(fake_train, test_size=0.1, random_state=42)\n","fake_train, fake_extra = train_test_split(fake_train, test_size=0.2, random_state=42)\n","\n","print(\"TESTING SET (20%)\")\n","print(f\"Real Test Files: {len(real_test)}\")\n","print(f\"Fake Test Files: {len(fake_test)}\")\n","\n","print(\"---------------------------------------------\")\n","\n","print(\"VALIDATION SET (10%)\")\n","print(f\"Real Validation Files: {len(real_val)}\")\n","print(f\"Fake Validation Files: {len(fake_val)}\")\n","\n","print(\"---------------------------------------------\")\n","\n","print(\"TRAINING SET (80%)\")\n","print(f\"Real Train Files: {len(real_train)}\")\n","print(f\"Fake Train Files: {len(fake_train)}\")\n","\n","print(\"---------------------------------------------\")\n","\n","\n","train_files = real_train + fake_train\n","val_files = real_val + fake_val\n","test_files = real_test + fake_test\n","\n","# shuffle training data\n","np.random.shuffle(train_files)\n","\n","print(f\"Total Train Files: {len(train_files)}\")\n","print(f\"Total Test Files: {len(test_files)}\")\n","print(f\"Total Validation: {len(val_files)}\")\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-12T15:13:27.453030Z","iopub.status.busy":"2024-05-12T15:13:27.452710Z","iopub.status.idle":"2024-05-12T15:13:27.460837Z","shell.execute_reply":"2024-05-12T15:13:27.459742Z","shell.execute_reply.started":"2024-05-12T15:13:27.453004Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of training samples: 1793\n","Number of validation samples: 250\n","Number of test samples: 623\n","train_steps_per_epoch: 57\n","val_steps_per_epoch 8\n","test_steps_per_epoch 20\n"]}],"source":["\n","# Calculate the number of steps per epoch for training and validation\n","train_steps_per_epoch = np.ceil(len(train_files) / batch_size).astype(int)\n","val_steps_per_epoch = np.ceil(len(val_files) / batch_size).astype(int)\n","test_steps_per_epoch = np.ceil(len(test_files) / batch_size).astype(int)\n","\n","# Calculate the number of samples in each set\n","num_train_samples = len(train_files)\n","num_val_samples = len(val_files)\n","num_test_samples = len(test_files)\n","\n","# Print the number of samples in each set\n","print(\"Number of training samples:\", num_train_samples)\n","print(\"Number of validation samples:\", num_val_samples)\n","print(\"Number of test samples:\", num_test_samples)\n","\n","print(\"train_steps_per_epoch:\", train_steps_per_epoch)\n","print(\"val_steps_per_epoch\", val_steps_per_epoch)\n","print(\"test_steps_per_epoch\", test_steps_per_epoch)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-12T15:13:27.463414Z","iopub.status.busy":"2024-05-12T15:13:27.462711Z","iopub.status.idle":"2024-05-12T16:21:28.354227Z","shell.execute_reply":"2024-05-12T16:21:28.351334Z","shell.execute_reply.started":"2024-05-12T15:13:27.463376Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# val_data = setup_dataset(DATA_DIR, real_val, fake_val, batch_size, is_training=False)\n","\n","val_data = pd.DataFrame()\n","val_data['frame'], val_data['label'] = setup_dataset(DATA_DIR, real_val, fake_val, batch_size, False)\n","val_data.head()\n","\n","val_data.to_csv('val_data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.355480Z","iopub.status.idle":"2024-05-12T16:21:28.356057Z","shell.execute_reply":"2024-05-12T16:21:28.355802Z","shell.execute_reply.started":"2024-05-12T16:21:28.355777Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# test_data = setup_dataset(DATA_DIR, real_test, fake_test, batch_size, is_training=False)\n","\n","test_data = pd.DataFrame()\n","test_data['frame'], test_data['label'] = setup_dataset(DATA_DIR, real_test, fake_test, batch_size, False)\n","test_data.head()\n","\n","test_data.to_csv('test_data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.358142Z","iopub.status.idle":"2024-05-12T16:21:28.359083Z","shell.execute_reply":"2024-05-12T16:21:28.358830Z","shell.execute_reply.started":"2024-05-12T16:21:28.358804Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["\n","# Set up training, validation, and test data\n","# train_data = setup_dataset(DATA_DIR, real_train, fake_train, batch_size, is_training=True)\n","\n","train_data = pd.DataFrame()\n","train_data['frame'], train_data['label'] = setup_dataset(DATA_DIR, real_train, fake_train, batch_size, True)\n","train_data = train_data.sample(frac = 1) ##shuffle\n","train_data.head()\n","\n","train_data.to_csv('train_data.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## Emotion Recognition Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.368871Z","iopub.status.idle":"2024-05-12T16:21:28.369863Z","shell.execute_reply":"2024-05-12T16:21:28.369647Z","shell.execute_reply.started":"2024-05-12T16:21:28.369621Z"},"trusted":true},"outputs":[],"source":["#======================================================================================\n","#                               CNN COMPONENT\n","#======================================================================================\n","\n","cnn_model = tf.keras.Sequential([\n","    Conv2D(8, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),  #8 filters, each 3x3, input images size 64x64x3(rgb channels), relu activation to output\n","    MaxPooling2D((2, 2)),   #max pooling with pool size 2x2 (selects max value in every 2x2 filter -> reduces spatial dimension by factor of 2\n","    Dropout(0.2),\n","    Conv2D(16, (3, 3), activation='relu'),  #16 filters, each 3x3, relu activation to output\n","    MaxPooling2D((2, 2)),      #max pooling with pool size 2x2 (selects max value in every 2x2 filter -> reduces spatial dimension by factor of 2\n","    Dropout(0.2),\n","    Conv2D(32, (3, 3), activation='relu'),  #32 filters, each 3x3, relu activation to output\n","    MaxPooling2D((2, 2)),       #max pooling with pool size 2x2 (selects max value in every 2x2 filter -> reduces spatial dimension by factor of 2\n","    Dropout(0.2),\n","    Conv2D(64, (3, 3), activation='relu'), #64 filters, each 3x3, relu activation to output\n","    MaxPooling2D((2, 2)),   #max pooling with pool size 2x2 (selects max value in every 2x2 filter -> reduces spatial dimension by factor of 2\n","    Conv2D(128, (3, 3), activation='relu'), #64 filters, each 3x3, relu activation to output\n","    MaxPooling2D((2, 2)),   #max pooling with pool size 2x2 (selects max value in every 2x2 filter -> reduces spatial dimension by factor of 2\n","    Flatten()       #flattens output into a 1D vector\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.371248Z","iopub.status.idle":"2024-05-12T16:21:28.371999Z","shell.execute_reply":"2024-05-12T16:21:28.371751Z","shell.execute_reply.started":"2024-05-12T16:21:28.371730Z"},"trusted":true},"outputs":[],"source":["#======================================================================================\n","#                               RNN COMPONENT (LSTM)\n","#======================================================================================\n","\n","#takes sequence of flattened vectors produced by the CNN and captures temporal info between them\n","\n","rnn_model = tf.keras.Sequential([\n","    LSTM(64)\n","])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.373146Z","iopub.status.idle":"2024-05-12T16:21:28.374270Z","shell.execute_reply":"2024-05-12T16:21:28.374020Z","shell.execute_reply.started":"2024-05-12T16:21:28.373998Z"},"trusted":true},"outputs":[],"source":["#======================================================================================\n","#                               FULL FER MODEL\n","#======================================================================================\n","\n","model = tf.keras.Sequential([\n","    TimeDistributed(cnn_model, input_shape=(seq_length, img_size, img_size, 3)),        #CNN MODEL\n","    rnn_model,      #LSTM\n","\n","    #===================================================\n","    #              FULLY CONNECTED LAYER\n","    #===================================================\n","\n","    #utilizes the extracted features and temporal information to learn complex relationships and transform the features into higher-level representations.\n","\n","    Dense(16, activation='relu'),       #fixed-length representation input from LSTM and applies ReLU activation function -> introduces non-linearity -> learn complex relationships between features.\n","    Dropout(0.4),       #prevent overfitting -> 20% of output set to 0\n","\n","    #===================================================\n","    #               OUTPUT LAYER\n","    #===================================================\n","\n","    #binary classification using sigmoid activation -> 0 or 1 -> probability of belonging to class\n","    #if value > 0.5 -> fake, if < 0.5 -> real\n","\n","    Dense(1, activation='sigmoid')\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.375247Z","iopub.status.idle":"2024-05-12T16:21:28.375744Z","shell.execute_reply":"2024-05-12T16:21:28.375523Z","shell.execute_reply.started":"2024-05-12T16:21:28.375503Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),     #adam optimizer with learning rate 0.0001\n","              loss='binary_crossentropy',       #loss function\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.377406Z","iopub.status.idle":"2024-05-12T16:21:28.377916Z","shell.execute_reply":"2024-05-12T16:21:28.377675Z","shell.execute_reply.started":"2024-05-12T16:21:28.377654Z"},"trusted":true},"outputs":[],"source":["# Callback for testing\n","\n","class TestCallback(tf.keras.callbacks.Callback):\n","    def on_train_end(self, logs=None):\n","        test_loss, test_acc = model.evaluate(test_data, steps=test_steps_per_epoch)\n","        print('Test loss:', test_loss)\n","        print('Test accuracy:', test_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.379837Z","iopub.status.idle":"2024-05-12T16:21:28.380895Z","shell.execute_reply":"2024-05-12T16:21:28.380721Z","shell.execute_reply.started":"2024-05-12T16:21:28.380703Z"},"trusted":true},"outputs":[],"source":["\n","class AccuracyPlotCallback(tf.keras.callbacks.Callback):\n","    def __init__(self):\n","        super(AccuracyPlotCallback, self).__init__()\n","        self.train_acc = []\n","        self.val_acc = []\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.train_acc.append(logs.get('accuracy'))\n","        self.val_acc.append(logs.get('val_accuracy'))\n","\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(self.train_acc, label='Training Accuracy')\n","        plt.plot(self.val_acc, label='Validation Accuracy')\n","        plt.title('Training and Validation Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.381759Z","iopub.status.idle":"2024-05-12T16:21:28.382614Z","shell.execute_reply":"2024-05-12T16:21:28.382438Z","shell.execute_reply.started":"2024-05-12T16:21:28.382421Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","class MetricsPlotCallback(tf.keras.callbacks.Callback):\n","    def on_train_end(self, logs=None):\n","        test_iterator = iter(test_data)\n","        y_true = []\n","        y_pred = []\n","        for _ in range(test_steps_per_epoch):\n","            X_test, y_test = next(test_iterator)\n","            y_true.extend(y_test)\n","            y_pred.extend((model.predict(X_test) > 0.5).astype(int).flatten())\n","        cm = confusion_matrix(y_true, y_pred)\n","        labels = ['Real', 'Deepfake']\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","        ax1 = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, square=True, xticklabels=labels, yticklabels=labels)\n","        ax1.set_xlabel('Predicted')\n","        ax1.set_ylabel('True')\n","        ax1.set_title('Confusion Matrix')\n","\n","        report = classification_report(y_true, y_pred, labels=np.unique(y_true), target_names=labels)\n","        ax2.text(0, 0.5, report, fontsize=12, verticalalignment='center')\n","        ax2.axis('off')\n","        ax2.set_title('Classification Report')\n","        \n","        plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.384526Z","iopub.status.idle":"2024-05-12T16:21:28.384881Z","shell.execute_reply":"2024-05-12T16:21:28.384729Z","shell.execute_reply.started":"2024-05-12T16:21:28.384714Z"},"trusted":true},"outputs":[],"source":["checkpoint = ModelCheckpoint(filepath='./Plots/celeb-df_model_plots_{epoch}.keras', monitor='val_loss', verbose=1, save_best_only=False, mode='auto', save_freq='epoch')\n","\n","        \n","metrics_plot_callback = MetricsPlotCallback()\n","\n","test_callback = TestCallback()\n","\n","accuracy_plot_callback = AccuracyPlotCallback()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:21:28.386227Z","iopub.status.idle":"2024-05-12T16:21:28.386658Z","shell.execute_reply":"2024-05-12T16:21:28.386501Z","shell.execute_reply.started":"2024-05-12T16:21:28.386485Z"},"trusted":true},"outputs":[],"source":["\n","# Train the model\n","history = model.fit(\n","    train_data,\n","    epochs=10,\n","    steps_per_epoch=train_steps_per_epoch,\n","    validation_data=val_data,\n","    validation_steps=val_steps_per_epoch,\n","    callbacks=[test_callback, checkpoint, accuracy_plot_callback, metrics_plot_callback]\n",")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":452468,"sourceId":854304,"sourceType":"datasetVersion"},{"datasetId":3120670,"sourceId":5380830,"sourceType":"datasetVersion"},{"datasetId":4917780,"sourceId":8280839,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
